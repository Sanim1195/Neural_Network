# 1

#  We use  activation functions because if the activation function itself is "non-linear", it allows for neural networks with usually two or more hidden layers to map nonlinear functions

#  your neural network will have two types of activation functions. The first will be the activation function used in hidden layers that can produce non linear outputs, and the second will be linear function used in the output layer.

# Usually,the activation function used for hidden neurons will be the same for all of them, but it doesnâ€™t have to.

# Write down the impact of tweaking Weights and  Bias can have on teh plot :